{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "import os\n",
        "\n",
        "def create_tar_gz_of_directory(directory_path, output_archive):\n",
        "    with tarfile.open(output_archive, \"w:gz\") as tar:\n",
        "        # Walk through the directory\n",
        "        for root, dirs, files in os.walk(directory_path):\n",
        "            for file in files:\n",
        "                # Create the path to your file\n",
        "                file_path = os.path.join(root, file)\n",
        "                # Calculate the arcname (name within the archive)\n",
        "                arcname = os.path.relpath(file_path, directory_path)\n",
        "                # Add the file to the archive; arcname controls the name inside the archive\n",
        "                tar.add(file_path, arcname=arcname)\n",
        "\n",
        "# Example usage\n",
        "directory_path = 'model'  # The directory to tar.gz\n",
        "output_archive = 'model.tar.gz'  # The output archive path\n",
        "create_tar_gz_of_directory(directory_path, output_archive)\n",
        "\n",
        "print(f\"Archive created at: {output_archive}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eu0GMGdoPSy6",
        "outputId": "e271190b-26e2-4533-ebd2-7d85689f331d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive created at: model.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Simple Client for the SageMaker Model Endpoint\n",
        "In this notebook, we go through a barebone, simplified implimentation of a client application for the MalConv model deployed on SageMaker. This client takes in the path of an executable file, then uses the EMBER library to extract its relevant features, which are then postprocessed to align with the expected input format of the model. It then uses the Boto3 library to establish a connection to AWS, authenticate us, and enable interactions with the SageMaker service that is serving our endpoint.\n",
        "\n",
        "As always, we start by taking care of a few dependencies:"
      ],
      "metadata": {
        "id": "iOGxMIPOktAQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4b0Dwrh3ocs",
        "outputId": "0a4fd949-a564-49b7-8325-3a60652ebbd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting awscli\n",
            "  Downloading awscli-1.32.71-py3-none-any.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.34.71-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore==1.34.71 (from awscli)\n",
            "  Downloading botocore-1.34.71-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docutils<0.17,>=0.10 (from awscli)\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.2/548.2 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting s3transfer<0.11.0,>=0.10.0 (from awscli)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<6.1,>=3.10 in /usr/local/lib/python3.10/dist-packages (from awscli) (6.0.1)\n",
            "Collecting colorama<0.4.5,>=0.2.5 (from awscli)\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting rsa<4.8,>=3.1.2 (from awscli)\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from botocore==1.34.71->awscli)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore==1.34.71->awscli) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore==1.34.71->awscli) (2.0.7)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.5.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.34.71->awscli) (1.16.0)\n",
            "Installing collected packages: rsa, jmespath, docutils, colorama, botocore, s3transfer, boto3, awscli\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9\n",
            "    Uninstalling rsa-4.9:\n",
            "      Successfully uninstalled rsa-4.9\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.18.1\n",
            "    Uninstalling docutils-0.18.1:\n",
            "      Successfully uninstalled docutils-0.18.1\n",
            "Successfully installed awscli-1.32.71 boto3-1.34.71 botocore-1.34.71 colorama-0.4.4 docutils-0.16 jmespath-1.0.1 rsa-4.7.2 s3transfer-0.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install awscli boto3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ember"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S-smgzC4TaO",
        "outputId": "e1ad58f4-38df-4783-9311-1ad85e235254"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ember in /usr/local/lib/python3.10/dist-packages (0.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 1.23 is the latest that is compatible with the original EMBER code.\n",
        "!pip install numpy==1.23"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "XmoJIwhc6hSW",
        "outputId": "f0cb6d1e-872d-4709-d94f-c736c2697310"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23\n",
            "  Downloading numpy-1.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "8fbd5d6b0d2d4b45a2ba171da720e0f1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 0.12 is the latest that is compatible with the original EMBER code.\n",
        "!pip install lief==0.12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWNP_uiD4hbp",
        "outputId": "90d4c5a0-39cd-4ed2-a231-5b25821fb471"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lief==0.12 in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting the Executable File to Processed Feature Vectors\n",
        "\n",
        "This block of code is crucial for the process of preparing and submitting Portable Executable (PE) files for malware classification using a machine learning model hosted on AWS SageMaker. It encompasses two primary functions: extract_features and format_features, designed for extracting features from PE files using the EMBER feature extractor and formatting those features for compatibility with your machine learning model, respectively.\n",
        "\n",
        "**extract_features Function**\n",
        "This function takes the path to a PE file as its input and returns a feature vector extracted using the EMBER feature extraction methodology. The steps are as follows:\n",
        "\n",
        "*   PEFeatureExtractor Initialization: An instance of PEFeatureExtractor is created with a specified version (1 or 2), which determines the feature extraction method. The choice of version impacts the feature set and extraction behavior.\n",
        "*   Reading the PE File: The PE file is opened in binary mode (\"rb\"), and its contents are read into the variable bytez. This binary data is what the EMBER extractor operates on.\n",
        "*   Feature Extraction: The feature_vector method is called with the binary data of the PE file, returning a feature vector representing the file's characteristics from a cybersecurity perspective.\n",
        "\n",
        "**format_features Function**\n",
        "After extracting the features, they must be formatted correctly before submission to your machine learning model. This function performs such formatting:\n",
        "\n",
        "*   Array Conversion: The extracted features are converted into a NumPy array of type float32. This step ensures that the data is in a numerical format compatible with further processing and machine learning models.\n",
        "*   Tensor Conversion: The NumPy array is then converted into a PyTorch tensor of type long. This conversion is necessary because the model expects the input data in this specific format. Tensors are a fundamental data structure in PyTorch, allowing for efficient computations and easy integration with neural network models."
      ],
      "metadata": {
        "id": "JK5pxLjEj5es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from ember import read_vectorized_features, PEFeatureExtractor\n",
        "import io\n",
        "\n",
        "def extract_features(pe_file_path):\n",
        "    \"\"\"\n",
        "    Extract features from a PE file using the EMBER feature extractor.\n",
        "    \"\"\"\n",
        "    extractor = PEFeatureExtractor(2)  # The version parameter can be 1 or 2\n",
        "    with open(pe_file_path, \"rb\") as f:\n",
        "        bytez = f.read()\n",
        "    features = extractor.feature_vector(bytez)\n",
        "    return features\n",
        "\n",
        "def format_features(features):\n",
        "    \"\"\"\n",
        "    Formats the extracted features for the model. Adjust this function based\n",
        "    on how your model expects the input data.\n",
        "    \"\"\"\n",
        "    # This is a placeholder; adapt the formatting based on your model's needs\n",
        "    features = np.array(features, dtype=np.float32)\n",
        "    # Convert to a long tensor as expected by your model\n",
        "    features_tensor = torch.tensor(features, dtype=torch.long)\n",
        "    return features_tensor\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AK8pjXW4XwDr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell, we define the predict_with_sagemaker() function, which utilizes boto3 to (1) authenticate us to AWS, and (2) facilitate interactions with AWS services. The credentials for AWS are available from AWS Academy Learner Labs Panel -> AWS Details . Please note that these credentials change everytime the lab session is restarted.\n",
        "![](https://github.com/UNHSAILLab/S24-AISec/blob/main/Midterm%20Tutorial/AWSDetails.png?raw=true)\n",
        "\n",
        "Also, this implementation is taking the hard path, because my model interface is implemented to take in a serialized input to ensure fidelity in transmission - The easy path would be to define the model input interface to accept JSON objects. However, now that the model is deployed with our inference.py that already implements the hard way, we need to take an extra step here: use a buffer!\n",
        "\n",
        "The io.BytesIO class is used in Python as an in-memory bytes buffer. It behaves like a file object that can be read from and written to, but instead of reading from or writing to a physical file on the disk, it operates on an in-memory byte stream. This makes io.BytesIO particularly useful for cases where you need a file-like interface for data that doesn't necessarily need to be stored on disk, enabling faster read/write operations and reducing the need for disk I/O.\n",
        "\n",
        "Here's why io.BytesIO is used in the provided code and what it accomplishes:\n",
        "\n",
        "Efficient Data Serialization: When you need to serialize data (in this case, a PyTorch tensor) to a format that can be transmitted over a network or stored in a non-Python environment (like a SageMaker endpoint expecting byte streams), io.BytesIO provides a convenient way to capture that serialized data stream without needing to write to and read from a disk.\n",
        "\n",
        "Compatibility with File-like Interfaces: Many Python libraries, including torch.save for serializing PyTorch models or tensors, expect a file-like object for operations. io.BytesIO allows these libraries to operate on data in memory as if they were reading from or writing to a file, making it seamless to integrate with such libraries for in-memory operations.\n",
        "\n",
        "Network Transmission: When sending data over the network, such as submitting input features to a machine learning model hosted on SageMaker, the data needs to be in a byte format. io.BytesIO provides a straightforward way to convert complex Python objects (after serialization) into a byte stream that can be sent over the network.\n",
        "\n",
        "In the following code,\n",
        "\n",
        "*   A BytesIO buffer is explicitly created before calling torch.save. This buffer acts as an in-memory file, which torch.save can write to.\n",
        "\n",
        "*   The tensor features is saved to this buffer using torch.save(features, buffer).\n",
        "\n",
        "*   After saving, the buffer's pointer is reset to the start with buffer.seek(0). This step is necessary because after writing, the buffer's pointer will be at the end of the written content, so attempting to read or get the value without resetting it will result in an empty output.\n",
        "\n",
        "*   Finally, buffer.getvalue() is called to retrieve the byte stream content of the buffer, which is then sent in the request to the SageMaker endpoint."
      ],
      "metadata": {
        "id": "1q2rR-Gdf8oN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def submit_to_endpoint(endpoint_name, features):\n",
        "    \"\"\"\n",
        "    Submit the formatted features to the SageMaker endpoint for prediction.\n",
        "    \"\"\"\n",
        "    # Create a BytesIO buffer and save the tensor to this buffer\n",
        "    buffer = io.BytesIO()\n",
        "    torch.save(features, buffer)\n",
        "    buffer.seek(0)  # Move to the start of the buffer\n",
        "\n",
        "    runtime = boto3.client('sagemaker-runtime',\n",
        "                          aws_access_key_id='ASIAYS2NTIXOY35N4QFS',\n",
        "                          aws_secret_access_key='ceSCquVXdsiFHo+hlC7E86Z2U1/4962AZ+fDaNQR',\n",
        "                          aws_session_token='FwoGZXIvYXdzEK3//////////wEaDKmKuszU2WDDMripbCLOATY/C4+GKSvnyVZNdVxKj2A+dWZ0z4NZoHsVGP0lY1DoPMZHDInGFYczi6RjFoGuh1E9vxpY76L6FHNQN2L/olrvgHUlEHtzTMFbLhZM9eSyZZQgS2MKELB05j3fKEEMZGlWRvNgQXH0xjLxG7c7Vrqjtz8NnB196kj2G4AF0Y9R/fitgoAtvNTXyhv+j5wxqxNUO2PmxzDcAFVBQn9FmVMudGI4M3L4aH6SXF7u+pFamTtjOG513+pBEKpqopeZ9g5zgksBtgSxNInzIvuiKPqojrAGMi3r+trPAHd4iSf/oaU6LjrJIzlPOxEGhZkWjJn3azR2GpgckLqYD5hvAaS0J18='\n",
        "                          )\n",
        "    response = runtime.invoke_endpoint(\n",
        "        EndpointName=endpoint_name,\n",
        "        ContentType=\"application/octet-stream\",\n",
        "        Body=buffer.getvalue()  # Use the buffer's content\n",
        "    )\n",
        "    # Deserialize the response\n",
        "    result = json.loads(response['Body'].read().decode())\n",
        "    return result"
      ],
      "metadata": {
        "id": "C55rWJIMhLSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following step, we test the API endpoint by passing the extracted features of an executable file to the model, and displaying the prediction.\n",
        "The user-provided parameters in these model are (1) path of the executable file that you wish to classify, and (2) the name of the SageMaker endpoint serving your MalConv model. You can find the latter as follows: in the main navigation menu of the AWS SageMaker dashboard (left side), open the Inference category, and select the Endpoints item. It will take you to a page listing all of your endpoints. If you have multiple, you will probably want to use the one that is most recently created.\n",
        "![](https://github.com/UNHSAILLab/S24-AISec/blob/main/Midterm%20Tutorial/SageMakerEndPoints.png?raw=true)"
      ],
      "metadata": {
        "id": "0pXAIgbFg1TQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    pe_file_path = \"calc.exe\"\n",
        "    endpoint_name = \"pytorch-inference-2024-03-27-05-44-37-080\"\n",
        "\n",
        "    # Extract features from the PE file\n",
        "    features = extract_features(pe_file_path)\n",
        "\n",
        "    print (features)\n",
        "\n",
        "    # Format the features as required by the model\n",
        "    formatted_features = format_features(features)\n",
        "\n",
        "\n",
        "    # Submit the formatted features to the SageMaker endpoint\n",
        "    prediction = submit_to_endpoint(endpoint_name, formatted_features)\n",
        "    print(\"Prediction result:\", prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LySFy5kiX9sf",
        "outputId": "52de9938-c4a7-4b1f-b0fe-112067b37169"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
            "WARNING:   lief version 0.12.0-f8918911 found instead. There may be slight inconsistencies\n",
            "WARNING:   in the feature calculations.\n",
            "[0.5253462  0.00397283 0.0012429  ... 0.         0.         0.        ]\n",
            "Prediction result: [[0.999998927116394]]\n"
          ]
        }
      ]
    }
  ]
}