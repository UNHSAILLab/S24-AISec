{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment 1: Feature Engineering for Spam Classification\n",
        "\n",
        "**Objective:**\n",
        "Enhance the performance of the spam text message classifier by experimenting with feature engineering techniques. Your task is to compare the effectiveness of different feature representations and their impact on the classifier's performance.\n",
        "\n",
        "**Instructions:**\n",
        "1. **Count-Based Features:** Start with the CountVectorizer implementation from the tutorial as your baseline model.\n",
        "2. **TF-IDF Transformation:**\n",
        "   - Implement a version of the classifier that uses `TfidfVectorizer` for feature extraction. Analyze how TF-IDF features affect the model's accuracy, precision, recall, and F1 score compared to count-based features. For a one-page description of TF-IDF, [see this reference](https://tfidf.com/).\n",
        "3. **N-grams:**\n",
        "   - N-grams are an essential feature extraction technique in natural language processing that capture sequences of 'n' items from text, providing context that individual words (unigrams) alone cannot. By considering contiguous sequences of words, n-grams (where 'n' can be 2 for bi-grams, 3 for tri-grams, etc.) incorporate local word order and can significantly enhance the understanding of text semantics. For instance, in the sentence \"The quick brown fox jumps\", bi-grams would be sequences like \"The quick\" and \"quick brown\", capturing more contextual information than unigrams such as \"quick\", \"brown\". This additional context can be particularly valuable in tasks like spam detection, where specific word sequences may strongly indicate spam content. By adjusting the `ngram_range` parameter in text vectorization tools like `CountVectorizer` or `TfidfVectorizer`, one can experiment with the inclusion of n-grams to observe their impact on model performance, potentially improving the detection capabilities of classifiers by leveraging the richer linguistic context n-grams provide.\n",
        "   -- Implement another version of the classifier that includes bi-grams or tri-grams in the feature set. Adjust the `ngram_range` parameter in either CountVectorizer or TfidfVectorizer and observe the impact on the model's performance.\n",
        "\n",
        "**Submission Instructions:**\n",
        "- Submit your solution as a Jupyter Notebook (.IPynb file), as well as a link to your Google Colab notebook. Make sure that your Colab notebook is shared with \"anyone with link\".\n",
        "- Your notebook should include sections for each feature engineering strategy, clearly marked with comments or Markdown cells.\n",
        "- Each section should include the implementation of the feature extraction technique, model training, and evaluation.\n",
        "- Use accuracy, precision, recall, and F1 score as your evaluation metrics.\n",
        "- Include a summary section at the end of the notebook comparing the performance of the different feature engineering strategies.\n",
        "\n",
        "**Evaluation Metrics:**\n",
        "- Your assignment will be auto-graded based on the following metrics: accuracy, precision, recall, and F1 score. Ensure your notebook outputs these metrics for each model variant in a consistent and easily extractable format.\n",
        "\n",
        "**Note:** Ensure that each cell in your Jupyter Notebook that outputs the required metrics for auto-grading does so in a clear and consistent format. For instance, consider printing the metrics in a dictionary format:\n",
        "\n",
        "```python\n",
        "print({\"CountVectorizer -- Accuracy\": accuracy, \"CountVectorizer -- Precision\": precision, \"CountVectorizer -- Recall\": recall, \"CountVectorizer -- F1 Score\": f1})\n",
        "\n",
        "print({\"TFIDF -- Accuracy\": accuracy, \"TFIDF -- Precision\": precision, \"TFIDF -- Recall\": recall, \"TFIDF -- F1 Score\": f1})\n",
        "\n",
        "print({\"NGRAM -- Accuracy\": accuracy, \"NGRAM -- Precision\": precision, \"NGRAM -- Recall\": recall, \"NGRAM -- F1 Score\": f1})\n",
        "```\n",
        "\n",
        "This structure will help streamline the auto-grading process, ensuring that your submission can be evaluated efficiently and accurately.\n",
        "\n",
        "**Skeleton Code for the Assignment:**"
      ],
      "metadata": {
        "id": "ex2qWoI0brQ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSaIQI03bpxv"
      },
      "outputs": [],
      "source": [
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "# df = pd.read_csv('path_to_dataset.csv')\n",
        "\n",
        "# Split the dataset\n",
        "# X = df['text']\n",
        "# y = df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Baseline: CountVectorizer\n",
        "# vectorizer = CountVectorizer()\n",
        "# Implement your CountVectorizer feature extraction, model training, and evaluation here\n",
        "\n",
        "# TF-IDF Transformation\n",
        "# tfidf_vectorizer = TfidfVectorizer()\n",
        "# Implement your TfidfVectorizer feature extraction, model training, and evaluation here\n",
        "\n",
        "# N-grams\n",
        "# ngram_vectorizer = CountVectorizer(ngram_range=(1, 2)) # Example for using bi-grams\n",
        "# Implement your N-gram feature extraction, model training, and evaluation here\n",
        "\n",
        "# Summary\n",
        "# Compare the performance metrics of the different feature engineering strategies and summarize your findings.\n",
        "```"
      ]
    }
  ]
}